{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ0HC2v-CLtV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.utils import resample\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "file_path = '/content/drive/MyDrive/new_cleaned_labeled_data.csv'  # Update with your file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop redundant column if it exists\n",
        "if 'Unnamed: 0' in data.columns:\n",
        "    data.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "# Clean tweets\n",
        "def clean_tweet(tweet):\n",
        "    tweet = str(tweet)\n",
        "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
        "    tweet = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\s+\", \" \", tweet).strip()\n",
        "    return tweet\n",
        "\n",
        "data['tweet'] = data['corrected_tweet'].apply(clean_tweet)\n",
        "\n",
        "# Split dataset\n",
        "train_data, temp_data = train_test_split(data, test_size=0.3, stratify=data['class'], random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['class'], random_state=42)\n",
        "\n",
        "# Balance the dataset (Oversampling Hate Speech class)\n",
        "hate_speech = train_data[train_data['class'] == 0]\n",
        "train_data_balanced = pd.concat([\n",
        "    train_data,\n",
        "    resample(hate_speech, replace=True, n_samples=1000, random_state=42)\n",
        "])\n",
        "\n",
        "# Tokenizer (RoBERTa)\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "def tokenize_data(data):\n",
        "    return tokenizer(\n",
        "        data['tweet'].tolist(),\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "train_encodings = tokenize_data(train_data_balanced)\n",
        "val_encodings = tokenize_data(val_data)\n",
        "\n",
        "# Define Dataset class\n",
        "class HateSpeechDataset(Dataset):\n",
        "    def __init__(self, tokenized_texts, labels):\n",
        "        self.tokenized_texts = tokenized_texts\n",
        "        self.labels = labels.values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.tokenized_texts['input_ids'][idx],\n",
        "            'attention_mask': self.tokenized_texts['attention_mask'][idx],\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = HateSpeechDataset(train_encodings, train_data_balanced['class'])\n",
        "val_dataset = HateSpeechDataset(val_encodings, val_data['class'])\n",
        "\n",
        "# DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# Model (RoBERTa)\n",
        "model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
        "\n",
        "# Optimizer, scheduler, and class weights\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "scheduler = get_scheduler(\n",
        "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 3\n",
        ")\n",
        "\n",
        "class_weights = torch.tensor([3.0, 1.0, 2.0], dtype=torch.float).to('cuda')\n",
        "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Training Loop with Loss Tracking\n",
        "scaler = GradScaler()\n",
        "epochs = 3\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Tokenize test data\n",
        "test_encodings = tokenize_data(test_data)\n",
        "\n",
        "# Create test dataset and dataloader\n",
        "test_dataset = HateSpeechDataset(test_encodings, test_data['class'])\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Evaluate model on test data\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute normalized confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds, normalize='true')\n",
        "\n",
        "# Display normalized confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\n",
        "disp.plot(cmap='Blues', values_format=\".2f\")\n",
        "plt.title(\"Normalized Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(all_labels, all_preds, target_names=['Hate Speech', 'Offensive"
      ]
    }
  ]
}